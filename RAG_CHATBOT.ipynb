{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#loading text\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/humayra11/RAG-based-Chatbot-Company-Policies-/refs/heads/main/BDRCS%20HR%20POLICY.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    knowledge_text = response.text\n",
        "    print(\"Text loaded successfully from GitHub.\")\n",
        "else:\n",
        "    raise Exception(f\"Failed to load  text. Status code: {response.status_code}\")\n",
        "\n",
        "#Chunking\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(knowledge_text)\n",
        "print(f\"Number of chunks {len(chunks)}.\")\n",
        "\n",
        "#Embedding\n",
        "\n",
        "!pip install -q sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "chunk_embeddings = model.encode(chunks)\n",
        "\n",
        "#Vector Store with FAISS\n",
        "\n",
        "!pip install -q faiss-cpu\n",
        "import faiss, numpy as np\n",
        "\n",
        "d = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(np.array(chunk_embeddings).astype('float32'))\n",
        "\n",
        "print(f\"FAISS index created with {index.ntotal} vectors.\")\n",
        "\n",
        "#Load Generator Model\n",
        "!pip install -q transformers\n",
        "from transformers import pipeline\n",
        "generator = pipeline('text2text-generation', model='google/flan-t5-small')\n",
        "\n",
        "#Conversation History\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "#RAG Function with Memory and Citation\n",
        "def ans_ques(query):\n",
        "\n",
        "    query_embedding = model.encode([query]).astype('float32')\n",
        "    k = 2\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    past_context = \"\\n\\n\".join(\n",
        "        [f\"User: {h['query']}\\nBot: {h['ans']}\" for h in conversation_history[-3:]]\n",
        "    )\n",
        "\n",
        "    # Prompt for generation\n",
        "    prompt_template = f\"\"\"\n",
        "    Answer the question using only the context below.\n",
        "    If the answer is not in the context, say \"I don't have that information.\"\n",
        "\n",
        "    Previous conversation:\n",
        "    {past_context}\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    ques:\n",
        "    {query}\n",
        "\n",
        "    ans:\n",
        "    \"\"\"\n",
        "\n",
        "    # Genarate ans\n",
        "    response = generator(prompt_template, max_length=150)[0]['generated_text']\n",
        "\n",
        "    # Saving in conversation history\n",
        "    conversation_history.append({\"query\": query, \"ans\": response})\n",
        "\n",
        "    print(conversation_history)\n",
        "\n",
        "    #citation\n",
        "    citation_text = \" (Source: github_src)\"\n",
        "    final_ans = f\"{response}\\n\\n{citation_text}\"\n",
        "\n",
        "    return final_ans\n",
        "\n",
        "\n",
        "#Gradio Interface\n",
        "\n",
        "!pip install -q gradio\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(query):\n",
        "    ans = ans_ques(query)\n",
        "\n",
        "    if \"(Source:\" in ans:\n",
        "        main_ans, source = ans.split(\"(Source:\", 1)\n",
        "        source = \"(Source:\" + source\n",
        "    else:\n",
        "        main_ans, source = ans, \"No source found\"\n",
        "\n",
        "    history_text = \"\\n\\n\".join([f\"User: {h['query']}\\nBot: {h['ans']}\" for h in conversation_history])\n",
        "\n",
        "    return main_ans.strip(), source.strip(), history_text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask about company policy\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Chatbot ans\"),\n",
        "        gr.Textbox(label=\"Source\"),\n",
        "        gr.Textbox(label=\"Conversation History\", lines=10)\n",
        "    ],\n",
        "    title=\"RAG-based Chatbot\",\n",
        "    description=\"Retrieves answers and shows conversation memory.\"\n",
        ")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "JGlcpFgDXuTN",
        "outputId": "b50ea2fd-00e3-4b6c-cd63-6b35cb97e197"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text loaded successfully from GitHub.\n",
            "Number of chunks 170.\n",
            "FAISS index created with 170 vectors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c9c10430465bad262c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c9c10430465bad262c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}